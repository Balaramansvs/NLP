{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7kBBzR6V/yDhxLgWVetYb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Balaramansvs/NLP/blob/main/NLPBALA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "# -------------------\n",
        "# STEP 1: Dataset\n",
        "# -------------------\n",
        "custom_text = \"\"\"\n",
        "In a quiet village, there lived an old clockmaker.\n",
        "He spent his days repairing ancient timepieces and crafting new ones.\n",
        "One evening, he discovered a mysterious golden gear inside a broken watch.\n",
        "When he placed it into a clock, time began to flow backward.\n",
        "The village soon found itself living yesterday all over again.\n",
        "\"\"\"\n",
        "\n",
        "# -------------------\n",
        "# STEP 2: Preprocessing\n",
        "# -------------------\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    tokens = re.findall(r'\\b[a-z]+\\b', text)\n",
        "    return tokens\n",
        "\n",
        "tokens = preprocess(custom_text)\n",
        "\n",
        "# Train/Test Split\n",
        "random.seed(42)\n",
        "split_point = int(0.8 * len(tokens))\n",
        "train_tokens = tokens[:split_point]\n",
        "test_tokens = tokens[split_point:]\n",
        "\n",
        "# -------------------\n",
        "# STEP 3: Build Models\n",
        "# -------------------\n",
        "def build_unigram_model(tokens):\n",
        "    counts = Counter(tokens)\n",
        "    total_count = sum(counts.values())\n",
        "    return counts, total_count\n",
        "\n",
        "def build_bigram_model(tokens):\n",
        "    bigram_counts = Counter(zip(tokens[:-1], tokens[1:]))\n",
        "    unigram_counts = Counter(tokens)\n",
        "    return bigram_counts, unigram_counts\n",
        "\n",
        "unigram_counts, total_unigrams = build_unigram_model(train_tokens)\n",
        "bigram_counts, unigram_counts_for_bigram = build_bigram_model(train_tokens)\n",
        "vocab = set(train_tokens)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# -------------------\n",
        "# STEP 4: Perplexity Functions\n",
        "# -------------------\n",
        "def perplexity_unigram(test_tokens, prob_func):\n",
        "    N = len(test_tokens)\n",
        "    log_prob_sum = 0\n",
        "    for w in test_tokens:\n",
        "        prob = prob_func(w)\n",
        "        log_prob_sum += math.log(prob)\n",
        "    return math.exp(-log_prob_sum / N)\n",
        "\n",
        "def perplexity_bigram(test_tokens, prob_func):\n",
        "    N = len(test_tokens) - 1\n",
        "    log_prob_sum = 0\n",
        "    for i in range(N):\n",
        "        w1, w2 = test_tokens[i], test_tokens[i+1]\n",
        "        prob = prob_func(w1, w2)\n",
        "        log_prob_sum += math.log(prob)\n",
        "    return math.exp(-log_prob_sum / N)\n",
        "\n",
        "# -------------------\n",
        "# STEP 5: Smoothing Probability Functions\n",
        "# -------------------\n",
        "# No smoothing\n",
        "def prob_uni_no(w):\n",
        "    return unigram_counts.get(w, 0) / total_unigrams if unigram_counts.get(w, 0) > 0 else 1e-8\n",
        "\n",
        "def prob_bi_no(w1, w2):\n",
        "    return bigram_counts.get((w1, w2), 0) / unigram_counts_for_bigram[w1] if bigram_counts.get((w1, w2), 0) > 0 else 1e-8\n",
        "\n",
        "# Laplace (Add-1)\n",
        "def prob_uni_laplace(w):\n",
        "    return (unigram_counts.get(w, 0) + 1) / (total_unigrams + vocab_size)\n",
        "\n",
        "def prob_bi_laplace(w1, w2):\n",
        "    return (bigram_counts.get((w1, w2), 0) + 1) / (unigram_counts_for_bigram[w1] + vocab_size)\n",
        "\n",
        "# Add-k\n",
        "k_value = 0.5\n",
        "def prob_uni_addk(w):\n",
        "    return (unigram_counts.get(w, 0) + k_value) / (total_unigrams + k_value * vocab_size)\n",
        "\n",
        "def prob_bi_addk(w1, w2):\n",
        "    return (bigram_counts.get((w1, w2), 0) + k_value) / (unigram_counts_for_bigram[w1] + k_value * vocab_size)\n",
        "\n",
        "# Good–Turing\n",
        "bigram_count_of_counts = Counter(bigram_counts.values())\n",
        "total_bigrams = sum(bigram_counts.values())\n",
        "def prob_bi_goodturing(w1, w2):\n",
        "    c = bigram_counts.get((w1, w2), 0)\n",
        "    if c < max(bigram_count_of_counts.keys()):\n",
        "        return ((c+1) * (bigram_count_of_counts.get(c+1, 0) / bigram_count_of_counts.get(c, 1))) / total_bigrams\n",
        "    else:\n",
        "        return c / total_bigrams\n",
        "\n",
        "# Kneser–Ney\n",
        "continuation_counts = Counter([w2 for (w1, w2) in bigram_counts.keys()])\n",
        "def prob_bi_kneserney(w1, w2, discount=0.75):\n",
        "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
        "    lambda_w1 = (discount / unigram_counts_for_bigram[w1]) * len([w for (u, w) in bigram_counts if u == w1])\n",
        "    p_continuation = continuation_counts[w2] / sum(continuation_counts.values())\n",
        "    return max(bigram_count - discount, 0) / unigram_counts_for_bigram[w1] + lambda_w1 * p_continuation\n",
        "\n",
        "# Witten–Bell\n",
        "unique_continuations = defaultdict(int)\n",
        "for (w1, w2) in bigram_counts.keys():\n",
        "    unique_continuations[w1] += 1\n",
        "\n",
        "def prob_bi_wittenbell(w1, w2):\n",
        "    T = unique_continuations[w1]\n",
        "    Z = vocab_size - T\n",
        "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
        "    if bigram_count > 0:\n",
        "        return bigram_count / (unigram_counts_for_bigram[w1] + T)\n",
        "    else:\n",
        "        return T / (Z * (unigram_counts_for_bigram[w1] + T))\n",
        "\n",
        "# -------------------\n",
        "# STEP 6: Evaluate All Methods\n",
        "# -------------------\n",
        "results = []\n",
        "methods = [\n",
        "    (\"No Smoothing\", prob_uni_no, prob_bi_no),\n",
        "    (\"Laplace\", prob_uni_laplace, prob_bi_laplace),\n",
        "    (\"Add-k\", prob_uni_addk, prob_bi_addk),\n",
        "    (\"Good–Turing\", prob_uni_no, prob_bi_goodturing), # Good–Turing not done for unigram here\n",
        "    (\"Kneser–Ney\", prob_uni_no, prob_bi_kneserney),   # Kneser–Ney is bigram only\n",
        "    (\"Witten–Bell\", prob_uni_no, prob_bi_wittenbell)  # Witten–Bell is bigram only\n",
        "]\n",
        "\n",
        "for name, uni_func, bi_func in methods:\n",
        "    try:\n",
        "        pp_uni = perplexity_unigram(test_tokens, uni_func)\n",
        "    except:\n",
        "        pp_uni = None\n",
        "    try:\n",
        "        pp_bi = perplexity_bigram(test_tokens, bi_func)\n",
        "    except:\n",
        "        pp_bi = None\n",
        "    results.append([name, pp_uni, pp_bi])\n",
        "\n",
        "# -------------------\n",
        "# STEP 7: Show Table\n",
        "# -------------------\n",
        "df = pd.DataFrame(results, columns=[\"Method\", \"Unigram Perplexity\", \"Bigram Perplexity\"])\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5NQglRsRtze",
        "outputId": "c32c6af9-66ce-4936-8970-374c83d8c57e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Method  Unigram Perplexity  Bigram Perplexity\n",
            "0  No Smoothing        2.637694e+07       1.000000e+08\n",
            "1       Laplace        7.605340e+01       3.809884e+01\n",
            "2         Add-k        1.122140e+02       3.819542e+01\n",
            "3   Good–Turing        2.637694e+07       1.000000e+00\n",
            "4    Kneser–Ney        2.637694e+07                NaN\n",
            "5   Witten–Bell        2.637694e+07                NaN\n"
          ]
        }
      ]
    }
  ]
}